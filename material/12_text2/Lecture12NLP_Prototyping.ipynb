{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**<center>Text as Data 2</center>**\n",
    "***<center>Datadriven Discovery</center>***\n",
    "\n",
    "<center>Snorre Ralund</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discovery and exploratory analysis of large text data \n",
    "### Agenda\n",
    "\n",
    "* Recap fundamentals.\n",
    "* Fundamentals continued: Words and representation. \n",
    "\n",
    "** \"Off-the-shelf\" tools for prototypical analysis **\n",
    "- Question driven vs Data-driven vs Modeldriven vs Tool-driven\n",
    "\n",
    "** Datadriven exploration **\n",
    "- Topic modelling and clustering in text\n",
    "- \"Free\" off-the-shelf tools for prototyping measurements in text.\n",
    "    - Lexical approaches.\n",
    "    - NLP Parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap: Fundamentals (1)\n",
    "-----\n",
    "\n",
    "What do we expect of **social** data scientists:\n",
    "- Computationally grounded analysis: \n",
    "    - Thourough exploratory analysis for both transparency and validation\n",
    "    - Knowing the breath and depth of your dataset. \n",
    "- Bias investigation before performance maximization.\n",
    "    - Is the bias equally distributed accross e.g. social groups\n",
    "    - How will it effect my measurements, \n",
    "- Ethical conscience:\n",
    "    - A reflexivity about model implications for e.g. privacy.        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap: Fundamentals (2)\n",
    "-----\n",
    "- Regular expressions: ** Define, Inspect, Refine, Repeat (DIR-R)** \n",
    "     - Systematic development and Transparency of the process.\n",
    "- Data validation and visualization: **Transparancy and quality control ** \n",
    "    - Random inspections (doc, sentence, context)\n",
    "    - Wordclouds and Wordtrees.\n",
    "- Descriptive analysis:\n",
    "    - Collocations and distinctive terms: PMI, $X^2$ and Mann-Whitney.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using the expected frequency\n",
    "If the probability of a word w, given y, p(w|y) is higher than we would expect we can use it to learn something about the data.\n",
    "\n",
    "**Measures**: $PMI$ , $X^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "####  Distinctive Terms \n",
    "** Learning about Classes or Subgroups in data ** \n",
    "\n",
    "## $X^2$ - chisquared\n",
    "The statistical co-occurence strengths of two or more events. \n",
    "Used to find Word-Phrases of two words being co-located next to each other (e.g. New York), or a word occurring when a certain group of people speak. \n",
    "\n",
    "Consider the contingency table O:\n",
    "\n",
    "\n",
    "|   | **O**  |   | bacon  |  $\\neg{bacon}$  |   |   |   |\n",
    "|---|---|---|----|:----:|---|---|---|\n",
    "|   | **vegan** |   | 10 | 990 |   |   |   |\n",
    "|   | **$\\neg{vegan}$** |   | 2500 | 10000 |   |   |   |\n",
    "|   |   |   |    |      |   |   |   |\n",
    "|   |   |   |    |      |   |   |   |\n",
    "    \n",
    "\n",
    "p(w=bacon|cat=vegan) = 10 / (990+10)\n",
    "\n",
    "p(w=bacon|cat=not_vegan) = 10 / (2500+10000)\n",
    "\n",
    "p(bacon) = (2500+10) / (10000+1000)\n",
    "\n",
    "$X^{2}=\\sum_{i, j} \\frac{\\left(O_{i j}-E_{i j}\\right)^{2}}{E_{i j}}$\n",
    "where $i$ ranges over rows of the table, $j$ ranges over columns, $O_{i j}$ is the observed value for cell ($i,j$) and $E_ij$ is the expected value.\n",
    "\n",
    "and \n",
    "$E_{i j} = N(p_i*p_j$)\n",
    "\n",
    "$X^2$ express the difference between the co-occurence we observe and what we would have expected to see if two events independent, relative to the latter. I.e. how many times more prevalent is the co-occurrence we observe than what we would expect if they were independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals (3) -  WORDS\n",
    "![](https://www.incimages.com/uploaded_files/image/970x450/getty_850704072_375370.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'are', 'the', 'boundaries', 'between-words', 'and', 'meaning?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"What are the boundaries between-words and meaning?\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WORDS (1)\n",
    "* How we \"split\" / locate words in text determines the number of dimensions\n",
    "    * Computational inefficiency \n",
    "    * Parameters are not shared among equivalent words.\n",
    "        * It makes a difference especially for low N tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Words (2)\n",
    "**ISSUES**\n",
    "* Spelling mistakes, weird uses of punctuation,\n",
    "* Emoticons: </3 , (:) , :-]\n",
    "* Multiwords: #no-more-work, New York, Federal Bureau of Finance, word/concept\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Representation **\n",
    "How to encode all relevant information in our tokens?\n",
    "* lower-casing: DO YOU REALLY WANT TO IGNORE MY ALLCAPS?!?!\n",
    "    * Our featurespace can potentially double if we don't lowercase.\n",
    "* Numbers: Infinite combinations of digits\n",
    "* Filtering: Which words to lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Words(3)\n",
    "** Solutions ** \n",
    "- ngrams collocations (PMI)\n",
    "- characterbased representation (e.g. LSTM)\n",
    "- subword tokenization (character ngrams)\n",
    "    - Good for language detection.\n",
    "    - Efficiency + captures natural similarity between different grammatical forms of the same word.\n",
    "    - **examples** : e.g. FastText - BERT: BPE (https://github.com/google/sentencepiece)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klatret©ªsen(Catch That Girl) is really great movie! It's a 'happy' movie. I watched this movie in 'Puchon International Fantastic Film Festival(PiFan)' on July 12nd, 2003. There is Action + Adventure + Comedy + Thrill + Happy + Romance(cute kids' love Triangle!). You must see this movie. :)\n"
     ]
    }
   ],
   "source": [
    "print(\"Klatret©ªsen(Catch That Girl) is really great movie! It's a 'happy' movie. I watched this movie in 'Puchon International Fantastic Film Festival(PiFan)' on July 12nd, 2003. There is Action + Adventure + Comedy + Thrill + Happy + Romance(cute kids' love Triangle!). You must see this movie. :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](tokenizer_similarity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](cv_tokenizer_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WORDS (4) - choosing tokenizer\n",
    "** depends on task **\n",
    "- no. of samples\n",
    "- do you need it to fit a pretrained model\n",
    "- how much subtlety do you need\n",
    "     - e.g. ['\"truth\"'] or ['\"' ,'truth','\"']\n",
    "\n",
    "\n",
    "I suggest you use (from the top3):\n",
    "- nltk.tokenize.causal.tweet\n",
    "- deepmoji (compatible with the deepmoji encoder we will use next term) \n",
    "- potts (simple and efficient script optimized for sentiment analysis)\n",
    "    - both are python2 based see below for a \"quickfix\" and download\n",
    "\n",
    "<pre><code> \n",
    "import nltk\n",
    "tweet_tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "tweet_tokenizer.tokenize('hello I speak emoticon and #hashtag :)'\n",
    "</code></pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### download potts tokenizer\n",
    "url = 'http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py'\n",
    "import requests\n",
    "script = requests.get(url).text\n",
    "# remove testing part in python2\n",
    "#section_marker = '###############################################################################'\n",
    "#script = section_marker.join(script.split(section_marker)[0:-1])\n",
    "# remove python2 code and superflous\n",
    "remove_parts = ['''# Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)''','import htmlentitydefs','''# Try to ensure unicode:\n",
    "        try:\n",
    "            s = unicode(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = str(s).encode('string_escape')\n",
    "            s = unicode(s)''']\n",
    "for part in remove_parts:\n",
    "    script = script.replace(part,'')\n",
    "script = script.replace('return words','return list(words)')\n",
    "script = script.split('    def tokenize_random_tweet(self):')[0]\n",
    "with open('potts_tokenizer.py','w',encoding='utf-8') as f:\n",
    "    f.write(script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## download deepmoji tokenizer\n",
    "url = 'https://raw.githubusercontent.com/bfelbo/DeepMoji/master/deepmoji/tokenizer.py'\n",
    "import requests\n",
    "script = requests.get(url).text\n",
    "with open('deepmoji_tokenizer.py','w',encoding='utf-8') as f:\n",
    "    f.write(script.replace(\"ur\\'\",\"r'\").replace(' = ur\"',' = r\"'))\n",
    "f.close()\n",
    "import deepmoji_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** depends on model / representation **\n",
    "- If model is sequential (e.g. lstm) smaller parts is better and more efficient.\n",
    "    * e.g. subwords: \"tion\",\"un\", \n",
    "    * \"don't\" or \"don\",\"'\".\"t\"\n",
    "- If model is based on BOW representation:\n",
    "    - Depends on the no. of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lexical approaches to text mining\n",
    "**Pros**\n",
    "* Comes with very low cost. \n",
    "* Fast and scalable.\n",
    "* Good for prototyping results.\n",
    "\n",
    "* Good for certain tasks (e.g. topic classification)\n",
    "\n",
    "** Example ** \n",
    "- 300 million documents, more 5 million unique tokens. How to inquire?\n",
    "\n",
    "![](https://raw.githubusercontent.com/snorreralund/sds_dump/master/toneDK.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* [Afinn (is danish!)](http://neuro.imm.dtu.dk/wiki/AFINN), \n",
    "* [Liu Hu (lexical)](http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) and \n",
    "* [Vader (lexical and rulebased)](https://github.com/cjhutto/vaderSentiment).\n",
    "\n",
    "\n",
    "* ** Purely Lexical ** Naively Matching positive words. *\"You are beautiful.\"* \n",
    "* ** Rule-based ** Can Adopt hard-coded rules to counter more or less simple negations. *\"You are not particularly beautiful.\"*\n",
    "\n",
    "** See notebook lexical_mining.ipynb for a compilation of lexicon classifiers. ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### issues with the lexical-based approach (1)\n",
    "\n",
    "*\"Pretty. Pretty actresses and actors. Pretty bad script. Pretty frequent \"let's strip to our undies\" scenes. Pretty fair F/X. Pretty jarring location decisions (the college dorm room looks like a high-end hotel room - probably because it was shot at a hotel). Pretty bland storyline. Pretty awful dialog. Pretty locations. Pretty annoying editing, unless you like the music video flash-cut style.This one isn't a guilty pleasure - this is more an embarrassing one. If you must watch this, pick a good dance/techno album and turn the sound off on the movie - you'll see the pretty people in their pretty black undies, and probably follow the story just fine.The cast may be able to act - I doubt that anyone could look skilled given the lines/plot that they had to deal with.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### issues with the lexical-based approach (2)\n",
    "- Atomized words: How well can meaning be derived from atomized words?  \n",
    "     - Not applicable to more complex rulebased versions:\n",
    "         - e.g. VADER\n",
    "         - Argument dictionary phrase based.\n",
    "       - Use in connection with Parsers: e.g. who is the emotion directed. \n",
    "- What is the Recall?\n",
    "    - Bad practice using dictionaries without explicit validation.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### use for prototyping! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### issues with the lexical-based approach (3)\n",
    "- Conglomerates of words \n",
    "   - What is the theoretical validity of a collection of words, scraped from many sources, validated at some historical point in time, given some score by a number people (students, amazonturks)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Construct Validity in (Automated) Content Analysis (2)\n",
    "Krippendorf 2004: *\"Content Analysis\"*\n",
    "\n",
    "- When constructing a variable from an interpretation of text (as opposed to survey, or registerbased research).\n",
    "    - We have the *possibility*,\n",
    "    - and the *obligation* to show that our interpretation are **correct**.\n",
    "    - be **precise**.\n",
    "        - What is it that we are measuring?\n",
    "        - What are we not measuring?\n",
    "    - situate it in contexts.\n",
    "        - E.g negative expressions at a soccer match, or at a workplace should potentially be treated differently in the analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Construct Validity in (Automated) Content Analysis (2)\n",
    "- Define, delimit and expose the variation in expression, and context.\n",
    "    - Give examples.\n",
    "    - Border cases / hard calls.\n",
    "- Show that you understand your corpus and the context that the communication is made *within*.\n",
    "    - Do this in a datadriven manner.\n",
    "    \n",
    "    - Compare to other dictionaries and show in the context of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discovery and datadriven learning of text data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploratory analysis on high dimensional data\n",
    "- 300 million documents, more than 5 million unique tokens. How to inquire?\n",
    "\n",
    "** Clustering to reduce dimensionality and efficiently seek out the variation in your corpus ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. inquiry: I want to learn about my data. - Not overlook important and interesting phenomenons.\n",
    "\n",
    "2. Omitted variable bias:\n",
    "    * e.g. Expressions of sentiment cannot be viewed as uneffected by.\n",
    "    - Should be seen in context with other phenomenons.\n",
    "    \n",
    "3. Grounding your theoretical construct: What are you measuring?\n",
    "\n",
    "\n",
    "Annotation/Coding scheme/categorizatino/theoretical categorization is fundamentally controversial. \n",
    "Unless you deal with a phenomenon so common sense that \"everyone\" can recognize it.\n",
    "Therefore performance in relation to a human gold standard, is not meaningful without a thorough and empirically grounded argument/proof that you are an expert in all its manifestations / ambivalences and problems. \n",
    "+ ** Transparency ** in relation to this. \n",
    " \n",
    "What is performance? What is aggreement with human annotators? If no theoretical validity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelbased discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topic modelling\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*pZo_IcxW1GVuH2vQKdoIMQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modelling (2)\n",
    "- Widely used tool in both research and industry. \n",
    "    - powering search algorithms, document retrieval, and recommendation systems. \n",
    "\n",
    "- Used for both measurement and discovery in the Social Sciences. \n",
    "\n",
    "** Pros ** \n",
    "- Praised for its inductive and datadriven properties.\n",
    "    - I.e. we did not come to the data with preconcieved theoretical ideas about what exists and what is important.\n",
    "- Beyond atomized words, and can handle polysemi of words.\n",
    "    - But still based on the BOW assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative models (1)\n",
    "- Define a model that you believe describe the data generation process.\n",
    "    - E.g. which parameters determine the probability of a network tie,\n",
    "    - Word in a document.\n",
    " \n",
    "- Define the variables and their dependencies.\n",
    "    - Network: Same school, ethnicity, culture, gender. \n",
    "    - Words: Mood, speaker, social situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative models (2)\n",
    "\n",
    "### Naive Bayes : p(x,y) \n",
    "\n",
    "Simple generative model for the probability of a drawing \"word\" given a categorical variable. \n",
    "\n",
    "** Example ** \n",
    "\n",
    "p(w=Yes | y=tired)\n",
    "\n",
    "Following me every morning we can observe and approximate p(yes|tired)\n",
    "\n",
    "- Using bayes rule\n",
    "- p(y|x) = p(x|y)*p(x) / p(y)\n",
    "- We observe p(x|y), p(y) and p(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative models (3)\n",
    "### LDA\n",
    "- Variables in the generative model does not have to be observable.\n",
    "    - They can be **latent** (similar to neural networks)\n",
    "\n",
    "Model definition:\n",
    "- The probability of a \"drawing\" a word is dependent on the topic.\n",
    "    - Topics are **Latent** - i.e. not observed.\n",
    "    - Topics are distributions of word probabilities.\n",
    "        - Words can be present in more than one topic.\n",
    "- Documents consist of multiple topics.\n",
    "\n",
    "** Can be extended with any latent structure as well as known variable **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1500/0*II7wZlKViCt4ssBm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "** Latent Dirichlet Allocation ** \n",
    "\n",
    "$p\\left(\\beta_{1 : K}, \\theta_{1 : D}, z_{1 : D}, w_{1 : D}\\right)$\n",
    "\n",
    "$=\\prod_{i=1}^{K} p\\left(\\beta_{i}\\right) \\prod_{d=1}^{D} p\\left(\\theta_{d}\\right)$\n",
    "\n",
    "$\\quad\\left(\\prod_{n=1}^{N} p\\left(z_{d, n} | \\theta_{d}\\right) p\\left(w_{d, n} | \\beta_{1 : K}, z_{d, n}\\right)\\right)$\n",
    "\n",
    "$\\beta_{1 : K}$ distribution of topics. \n",
    "\n",
    "where each $\\beta_{K}$ is a ditribution over the vocabulary\n",
    "\n",
    "\n",
    "the per-document topic proportions\n",
    "$\\theta_{d} .$ \n",
    "\n",
    "where $\\theta_{d, k}$ is the topic proportion\n",
    "for topic $k$ in document $d$ \n",
    "\n",
    "topic assignment $z_{d, n}$ of the n-th word in the document.\n",
    "\n",
    "the observed word $w_{d, n}$ depends on the topic assignment $z_{d, n}$ and all of the topics $\\beta_{1 : K}$\n",
    "\n",
    "\n",
    "** actually $\\theta$ and $\\beta$ are superflous if we know the topic assignments $z_{d_n}$ ** \n",
    "collapsed gibbs sampling approach. Hence what is called a gibbs sampling procedure, where.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Useful Models\n",
    "*\"All model are wrong, but some are useful\"*\n",
    "- We did actually come with preconceived theoretical ideas. \n",
    "    - How inductive and datadriven is the model really?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Still remember \n",
    "**Anscombe famous example on why you need to plot your data**\n",
    "<img src=\"https://d2f99xq7vri1nk.cloudfront.net/Anscombe_1_0_0.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Models with unrealistic assumptions can produce wildly misleading results ** \n",
    "\n",
    "Lanchichetti et. al. 2014:\n",
    "\n",
    "LDA collapses different languages into the same cluster given a wrong prior $\\alpha$ prior.\n",
    "\n",
    "** Complex models are hard to fit **\n",
    "- Instability of solutions and local minima. \n",
    "    - (Lancietti et al. 2014; Chuang et al. 2015, Roberts et al. 2016, Wilkerson and Casas 2017; Gentzkow et al. 2017: 27; Agrawal et al. 2018), \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Anscombe's dinosaur friend.**\n",
    "[Figure taken from here](https://www.autodeskresearch.com/publications/samestats)\n",
    "\n",
    "![](https://d2f99xq7vri1nk.cloudfront.net/DinoSequentialSmaller.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To the rescue (the 2020 model)\n",
    "![](https://article.images.consumerreports.org/prod/content/dam/CRO%20Images%202018/Cars/October/CR-Cars-InlineHero-2018-Tesla-Model-3-f-driving-10-18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** a network based model ** - 2018 model\n",
    "\n",
    "Hierarchical Stochastic Block Modelling - hSBM \n",
    "- Gerlach, Peixoto and Altmann 2018\n",
    "\n",
    "Less assumption and more adaptive: \n",
    "- You do not need to specify K (no. of topics).\n",
    "- You do not need specify a prior for $alpha$ determining the size distribution of topics.\n",
    "\n",
    "\n",
    "- Not as many prebuilt tools around it.\n",
    "- Hard to install (use docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prototyping(2)\n",
    "- Remember that this model is also wrong. \n",
    "- Don't use as final measurement model without explicit validation.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
