{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**<center>Text as Data 3</center>**\n",
    "***<center>Text Classification </center>***\n",
    "\n",
    "<center>Snorre Ralund</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap (1)\n",
    "- Tokenization for the task\n",
    "    - Social media tokenizers,\n",
    "    - Formal text tokenizers (e.g. msbp)\n",
    "    - Subword tokenization for deep learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap (2)\n",
    "- Prototyping. \n",
    "- The ability to see different angles of your corpus.\n",
    "    - Before we can trust the machine doing the interpretation, you need to demonstrate that you understand the data and the categorizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap (3)\n",
    "** Methods** \n",
    "- Lexical methods. \n",
    "    - Easy prototyping.\n",
    "    - Useful for topic classification\n",
    "    - Build lexicons yourself using active learning [(King, Lam and Robert 2017)](https://gking.harvard.edu/publications/computer-assisted-keyword-and-document-set-discovery-fromunstructured-text) and word similarity search [(e.g. Marquez et al. 2016)](https://www.cs.waikato.ac.nz/~eibe/pubs/emo_lex_wi.pdf).\n",
    "    - Use for locating \"Hard predictions\" to learn about your ML classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap (4)\n",
    "\n",
    "- Topic modelling\n",
    "    - Used to be state-of-the-art..\n",
    "    - Very strong assumptions about the data. (No. of topics, size of topics, concentration of topics)\n",
    "    - Instability\n",
    "\n",
    "\n",
    "- Clustering still important for exploration\n",
    "- --> Use [hSBM](https://advances.sciencemag.org/content/4/7/eaaq1360), which is much more expressive than the LDA generative model.\n",
    "    - network based approach\n",
    "    - [Github](https://github.com/martingerlach/hSBM_Topicmodel)\n",
    "- Compare topics to Classifications for a richer understanding of the measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## example of the hsbm and lexical combined.\n",
    "import pickle\n",
    "t2w = pickle.load(open('hsbm_t2w.pkl','rb'))\n",
    "corr_mat = pickle.load(open('corr_mat_ds_topic+lexicon.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', 0.9349804941482445),\n",
       " ('losses', 0.05071521456436931),\n",
       " ('hinge', 0.014304291287386216)]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loss function topic\n",
    "t2w[355]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive', 0.31085587254538716),\n",
       " ('negative', 0.3008521674694331),\n",
       " ('false', 0.1663579103371619),\n",
       " ('positives', 0.05668766209707299),\n",
       " ('proportion', 0.051871063356798815),\n",
       " ('negatives', 0.04520192663949611),\n",
       " ('coverage', 0.014820303816228233),\n",
       " ('wrongly', 0.01148573545757688),\n",
       " ('obs', 0.00889218228973694),\n",
       " ('cut-off', 0.00889218228973694),\n",
       " ('ks', 0.005928121526491293),\n",
       " ('prevalence', 0.003705075954057058),\n",
       " ('analog', 0.003705075954057058),\n",
       " ('culture', 0.002593553167839941),\n",
       " ('48.', 0.002593553167839941),\n",
       " ('class_names', 0.0011115227862171174),\n",
       " ('uncalibrated', 0.0011115227862171174),\n",
       " ('a-d', 0.0007410151908114116),\n",
       " ('custom-made', 0.0007410151908114116),\n",
       " ('fertility', 0.0007410151908114116)]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation topic\n",
    "t2w[306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('missing', 0.3007017543859649),\n",
       " ('replace', 0.11368421052631579),\n",
       " ('median', 0.06701754385964913),\n",
       " ('fill', 0.05824561403508772),\n",
       " ('nan', 0.05578947368421053),\n",
       " ('empty', 0.039649122807017545),\n",
       " ('imputation', 0.034736842105263156),\n",
       " ('impute', 0.030877192982456142),\n",
       " ('replacing', 0.025964912280701753),\n",
       " ('coded', 0.024912280701754386),\n",
       " ('filled', 0.023859649122807018),\n",
       " ('filling', 0.018596491228070177),\n",
       " ('imputing', 0.015438596491228071),\n",
       " ('interpolate', 0.012982456140350877),\n",
       " ('gaps', 0.012982456140350877),\n",
       " ('nans', 0.01087719298245614),\n",
       " ('mice', 0.010175438596491228),\n",
       " ('imputed', 0.010175438596491228),\n",
       " ('inf', 0.009824561403508772),\n",
       " ('nas', 0.009473684210526316)]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputation topic: \n",
    "t2w[242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('validation', 0.25963302752293577),\n",
       " ('overfitting', 0.14977064220183486),\n",
       " ('overfit', 0.06674311926605504),\n",
       " ('generalize', 0.05045871559633028),\n",
       " ('unseen', 0.04908256880733945),\n",
       " ('early', 0.0426605504587156),\n",
       " ('generalization', 0.038073394495412846),\n",
       " ('stopping', 0.027522935779816515),\n",
       " ('over-fitting', 0.0213302752293578),\n",
       " ('monitor', 0.012844036697247707),\n",
       " ('underfitting', 0.011238532110091744),\n",
       " ('generalise', 0.010091743119266056),\n",
       " ('regularize', 0.010091743119266056),\n",
       " ('dev', 0.009403669724770643),\n",
       " ('generalisation', 0.009174311926605505),\n",
       " ('overfits', 0.00871559633027523),\n",
       " ('generalizes', 0.008486238532110092),\n",
       " ('overfitted', 0.008486238532110092),\n",
       " ('generalizing', 0.007568807339449542),\n",
       " ('bias-variance', 0.006192660550458716)]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overfitting topic\n",
    "t2w[259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('can', 0.3396476729012614),\n",
       "  ('use', 0.219377990430622),\n",
       "  ('need', 0.11133101348412353),\n",
       "  ('see', 0.0871900826446281),\n",
       "  ('add', 0.034167029143105695),\n",
       "  ('check', 0.030643758155719878),\n",
       "  ('solution', 0.030295780774249673),\n",
       "  ('called', 0.025511091779034364),\n",
       "  ('directly', 0.015876468029578077),\n",
       "  ('easily', 0.014810787298825576),\n",
       "  ('option', 0.013331883427577207),\n",
       "  ('allows', 0.010526315789473684),\n",
       "  ('provided', 0.010026098303610266),\n",
       "  ('parts', 0.008155719878207917),\n",
       "  ('suggested', 0.006894301870378426),\n",
       "  ('alternative', 0.006872553284036537),\n",
       "  ('inside', 0.006763810352327098),\n",
       "  ('specify', 0.0063723357981731185),\n",
       "  ('modify', 0.004023488473249239),\n",
       "  ('wish', 0.0036537625054371466)],)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Positive correlation\n",
    "# see easy solution\n",
    "t2w[10],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('et', 0.011533894951094024),\n",
       " ('al', 0.011364278848872053),\n",
       " ('david', 0.002883473737773506),\n",
       " ('bengio', 0.0023180867303669362),\n",
       " ('arxiv', 0.0020353932266636513),\n",
       " ('crossvalidated', 0.0019788545259229943),\n",
       " ('van', 0.0018092384237010234),\n",
       " ('2004', 0.0016961610222197094),\n",
       " ('2006', 0.0016961610222197094),\n",
       " ('summarized', 0.0015265449199977385),\n",
       " ('2003', 0.0014700062192570815),\n",
       " ('ieee', 0.0014134675185164245),\n",
       " ('integral', 0.0014134675185164245),\n",
       " ('2001', 0.0011873127155537966),\n",
       " ('yoshua', 0.0011873127155537966),\n",
       " ('der', 0.0011307740148131396),\n",
       " ('christopher', 0.0010742353140724826),\n",
       " ('friedman', 0.0010742353140724826),\n",
       " ('vol', 0.0010176966133318257),\n",
       " ('li', 0.0010176966133318257)]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference topic\n",
    "t2w[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 0.4259201979585524),\n",
       " ('library', 0.14444788122486854),\n",
       " ('scikit-learn', 0.09078255490256727),\n",
       " ('libraries', 0.0838230745437674),\n",
       " ('scikit', 0.03897309000927931),\n",
       " ('pipeline', 0.03294154036498608),\n",
       " ('scipy', 0.02551809464893288),\n",
       " ('supports', 0.02381688833900402),\n",
       " ('++', 0.022888957624497372),\n",
       " ('stackoverflow', 0.01840395917104856),\n",
       " ('syntax', 0.012681719764924219),\n",
       " ('implements', 0.011599133931333128),\n",
       " ('thread', 0.010516548097742036),\n",
       " ('integrate', 0.01005258274048871),\n",
       " ('reproducible', 0.009588617383235385),\n",
       " ('cheers', 0.00912465202598206),\n",
       " ('readable', 0.006804825239715435),\n",
       " ('checkout', 0.005258274048871018),\n",
       " ('fantastic', 0.0038663779771110425),\n",
       " ('extensions', 0.0037117228580266005)]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pacakge library topic\n",
    "t2w[91]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda \n",
    "### Text classification for measurement\n",
    "\n",
    "** Adoption of state-of-the-art ** \n",
    "- Setting up a good baseline for understanding progress.\n",
    "    - Baselines and (tri-)bigrams.\n",
    "\n",
    "\n",
    "- Transfer learning\n",
    "    - Noisy labels.\n",
    "    - Language models.\n",
    "\n",
    "** Methodological issues **\n",
    "- Bias in NLP systems and language models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baselines\n",
    "- How good is a 90 accuracy score mean?\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*unP1qDkUPSSUsa2-530zEg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Baselines (1)\n",
    "- Understanding your progress with good baselines.\n",
    "    - Sanity checks\n",
    "    \n",
    "    - Testing your models on known datasets.\n",
    "    \n",
    "    - Compare to state-of-the-art.\n",
    "\n",
    "\n",
    "- You need good baselines to navigate the many choices you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Baselines (2)\n",
    "- Understading the complexity of the task\n",
    "    - Test your model in relation to the \"Best\" BOW.\n",
    "    \n",
    "    \n",
    "- Wang and Manning 2012: [\"Baselines and Bigrams: Simple, Good Sentiment and Topic Classification\"](https://www.aclweb.org/anthology/P12-2018) \n",
    "    - Combine the Naive Bayes with an SVM (NBSVM)\n",
    "    - Use Naive Bayes as feature selector, and perform simple regularized linear regression. \"NBLOG\"\n",
    "\n",
    "$\\mathbf{p}=\\alpha+\\sum_{i : y^{(i)}=1} \\mathbf{f}^{(i)}$\n",
    "\n",
    "$\\mathbf{q}=\\alpha+\\sum_{i : y^{(i)}=-1} \\mathbf{f}^{(i)}$\n",
    "\n",
    "$\\mathbf{r}=\\log \\left(\\frac{\\mathbf{p} /\\|\\mathbf{p}\\|_{1}}{\\mathbf{q} /\\|\\mathbf{q}\\|_{1}}\\right)$\n",
    "\n",
    "Where $\\alpha$ is the laplace smoothing parameter and $\\mathbf{f}^{(i)}$ is a count of feature $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Naive Bayes Ratio as input **\n",
    "- how much more probable is the word \"bacon\" in the positive reviews, than the negative review?\n",
    "\n",
    "**or even better with trigrams**\n",
    "- how much more probable is the trigram \"not enough bacon\" in the negative reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FastText baseline\n",
    "Joulin et al. 2016: [\"Bag of Tricks for Efficient Text Classification\"](https://arxiv.org/abs/1607.01759)\n",
    "\n",
    "- Simple 2 layer neural network input -->(embedding,softmax). \n",
    "- Good for multi-label classification\n",
    "    - Can leverage label correlation by sharing parameters in the embeddings of each word.\n",
    "\n",
    "![](https://wiki.math.uwaterloo.ca/statwiki/images/2/25/model_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Download fastText for command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#! wget https://github.com/facebookresearch/fastText/archive/v0.2.0.zip\n",
    "#! unzip v0.2.0.zip\n",
    "#! cd fastText-0.2.0\n",
    "#! make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Download benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download one of the benchmark dataset\n",
    "ag_df = pd.read_csv('https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/train.csv',header=None\n",
    "                    ,names=['y','title','text'])\n",
    "ag_df_test = pd.read_csv('https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/test.csv',header=None\n",
    "                    ,names=['y','title','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import IMDB\n",
    "##IMDB DATA\n",
    "# getting the imdb review data.\n",
    "import requests\n",
    "response = requests.get('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',stream=True)\n",
    "with open('aclImdb_v1.tar.gz','wb') as f:\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "            #f.flush() # forces it to write on disk \n",
    "\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "import os,pandas as pd\n",
    "import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "#pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "with tqdm.tqdm(total=50000) as t:\n",
    "    for s in ('test', 'train'):\n",
    "        for l in ('pos', 'neg'):\n",
    "            path = os.path.join(basepath, s, l)\n",
    "            for file in os.listdir(path):\n",
    "                with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n",
    "                    txt = infile.read()\n",
    "                    df = df.append([[txt, labels[l],s]],ignore_index=True)\n",
    "                    #pbar.update()\n",
    "                    t.update()\n",
    "df.columns = ['review', 'sentiment','set']\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "imdb_df = df.reindex(np.random.permutation(df.index))\n",
    "imdb_df.to_csv('imdb_sentiment.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Load IMDB: One of the benchmark from the Baselines and Bigrams\n",
    "import pandas as pd\n",
    "imdb_df = pd.read_csv('imdb_sentiment.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Movie Review example: Benchmark from Kim et al. 2014: Convolutional Neural Networks for sentence classification\n",
    "import requests\n",
    "neg = 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg'\n",
    "pos = 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos'\n",
    "neg = requests.get(neg).text.split('\\n')\n",
    "pos = requests.get(pos).text.split('\\n')\n",
    "docs = neg+pos\n",
    "# Make train test split\n",
    "idx = np.random.permutation(np.arange(len(tokenized)))\n",
    "\n",
    "n = int(len(idx)*0.75)\n",
    "train,test = idx[0:n],idx[n:]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "y = np.array(([0]*len(neg))+([1]*len(pos)))\n",
    "\n",
    "df = pd.DataFrame({'text':docs,'y':y})\n",
    "train_df,test_df = df.iloc[train],df.iloc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Format data for fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def make_fasttext_format(df,y_col,text_col,outfile,tokenizer=nltk.tokenize.TweetTokenizer().tokenize):\n",
    "    docs = df[text_col].values\n",
    "    # tokenize\n",
    "    tokenized = [' '.join(tokenizer(doc)) for doc in docs]\n",
    "    # lower\n",
    "    tokenized = [doc.lower().replace('\\n',' __newline__ ') for doc in tokenized]\n",
    "    fasttext_labels = ['__label__%s'%str(i) for i in df[y_col]]\n",
    "    fast_docs = [' '.join([fasttext_labels[i],tokenized[i]]) for i in range(len(df))]\n",
    "    with open(outfile,'w') as f:\n",
    "        f.write('\\n'.join(fast_docs))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "make_fasttext_format(ag_df,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='ag_train')\n",
    "make_fasttext_format(ag_df_test,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='ag_test')\n",
    "\n",
    "make_fasttext_format(imdb_df[imdb_df['set']=='train'],\n",
    "                     y_col='sentiment',\n",
    "                     text_col='review'\n",
    "                     ,outfile='imdb_train')\n",
    "make_fasttext_format(imdb_df[imdb_df['set']=='test'],\n",
    "                     y_col='sentiment',\n",
    "                     text_col='review'\n",
    "                     ,outfile='imdb_test')\n",
    "make_fasttext_format(train_df,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='mr_train')\n",
    "make_fasttext_format(test_df,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='mr_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run fastText classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 7M words\n",
      "Number of words:  103093\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1269315 lr:  0.000000 loss:  0.030434 ETA:   0h 0m\n",
      "N\t25000\n",
      "P@1\t0.9\n",
      "R@1\t0.9\n"
     ]
    }
   ],
   "source": [
    "# set path to fasttext\n",
    "fast_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/fastText-0.2.0'\n",
    "! {fast_path}/./fasttext supervised -input imdb_train -output model_fast -lr 0.5 -epoch 50 -wordNgrams 3 -dim 10 -ws 5\n",
    "#! {fast_path}/./fasttext supervised -input fasttext.train -output model_fast -lr 0.5 -epoch 200 -wordNgrams 3 -dim 100 -ws 5\n",
    "! {fast_path}/./fasttext test model_fast.bin imdb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  78902\n",
      "Number of labels: 4\n",
      "Progress: 100.0% words/sec/thread: 1224338 lr:  0.000000 loss:  0.031434 ETA:   0h 0m\n",
      "N\t7600\n",
      "P@1\t0.912\n",
      "R@1\t0.912\n"
     ]
    }
   ],
   "source": [
    "### Ag example\n",
    "# set path to fasttext\n",
    "fast_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/fastText-0.2.0'\n",
    "! {fast_path}/./fasttext supervised -input ag_train -output model_fast -lr 0.5 -epoch 50 -wordNgrams 3 -dim 10 -ws 5\n",
    "#! {fast_path}/./fasttext supervised -input fasttext.train -output model_fast -lr 0.5 -epoch 200 -wordNgrams 3 -dim 100 -ws 5\n",
    "! {fast_path}/./fasttext test model_fast.bin ag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17808\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  987497 lr:  0.000000 loss:  0.022001 ETA:   0h 0m\n",
      "N\t2666\n",
      "P@1\t0.761\n",
      "R@1\t0.761\n"
     ]
    }
   ],
   "source": [
    "# set path to fasttext\n",
    "fast_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/fastText-0.2.0'\n",
    "! {fast_path}/./fasttext supervised -input mr_train -output model_fast -lr 0.5 -epoch 300 -wordNgrams 2 -dim 100 -ws 5\n",
    "#! {fast_path}/./fasttext supervised -input fasttext.train -output model_fast -lr 0.5 -epoch 200 -wordNgrams 3 -dim 100 -ws 5\n",
    "! {fast_path}/./fasttext test model_fast.bin mr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baselines and Bigrams implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Download script for searching the hyperparameters with bayesian optimization using the hyperopt package.\n",
    "import requests\n",
    "with open('get_bow_baseline.py','w') as f:\n",
    "    f.write(requests.get('https://raw.githubusercontent.com/snorreralund/test_tokenization/master/get_bow_baseline.py'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:07<00:00,  1.58s/it, best loss: 0.23339999999999994]\n",
      "Final accuracy and roc_auc score of tokenizer (nltk_tweet) + nb_log: 0.763 and 0.844\n"
     ]
    }
   ],
   "source": [
    "import get_bow_baseline as baseline\n",
    "baseline_mr = baseline.TokenizationTest(train_df,test_df,MAX_EVALS=150,scoring_function='accuracy_score')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.TweetTokenizer().tokenize\n",
    "baseline_mr.evaluate('nltk_tweet',tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anything Park Chan-wook creates is guaranteed ...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>... So some people might argue that this can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are not in the fairy tale of the naked empe...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this show about 3-4 years ago. It was da...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You know Jason, you know Freddy, and you know ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  y    set\n",
       "0  Anything Park Chan-wook creates is guaranteed ...  1   test\n",
       "1  ... So some people might argue that this can't...  0   test\n",
       "2  We are not in the fairy tale of the naked empe...  0  train\n",
       "3  I saw this show about 3-4 years ago. It was da...  1  train\n",
       "4  You know Jason, you know Freddy, and you know ...  0  train"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = imdb_df.drop('Unnamed: 0',axis=1)\n",
    "imdb_df.columns = ['text','y','set']\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [3:57:23<00:00, 116.67s/it, best loss: 0.09654400000000007] \n",
      "Final accuracy and roc_auc score of tokenizer (nltk_tweet) + nb_log: 0.890 and 0.953\n"
     ]
    }
   ],
   "source": [
    "import get_bow_baseline as baseline\n",
    "baseline_imdb = baseline.TokenizationTest(imdb_df[imdb_df['set']=='train'],imdb_df[imdb_df['set']=='test']\n",
    "                                          ,MAX_EVALS=100,scoring_function='accuracy_score')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.TweetTokenizer().tokenize\n",
    "baseline_imdb.evaluate('nltk_tweet',tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/31130754/transfer-learning.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**NLU is hard**\n",
    "* Natural Language Understanding does not come from a 1000 labeled examples.\n",
    "* Combinatorial space is extremely large\n",
    "    * > N different words\n",
    "    * >> N x N different bigram sequences\n",
    "    * >>> $N^k$ k-long sequences\n",
    "    * >> X? possible dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NLU needs many examples and detalied instructions \n",
    "\n",
    "\n",
    "###  NLU needs very powerful models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zero shot / few shot learning\n",
    "- Humans can understand things from small number of examples.\n",
    "    - Humans can recognize things they haven't seen before, but have been described to them.\n",
    "    - Before you understand the specific meaning of the word:\n",
    "        - you understand the context\n",
    "    - you know how to use it\n",
    "        - but not exactly its effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where to get labels?\n",
    "\n",
    "###  Two general innovations for leveraging large document collections \n",
    "-  Noisy labels\n",
    "-  Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noisy Labels\n",
    "- Leverage large scale user generated tags\n",
    "    - Hashtags for expressing topic and summarization.\n",
    "\t\t- #sarcasm #irony\n",
    "\t\t- #topic\n",
    "\t- Emojies to explicate emotional intention - DeepMoji\n",
    "\n",
    "** Other User generated tags ** \n",
    "- Subreddits - including reactions\n",
    "- Keywords in Scientific Articles\n",
    "- Tags in stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Models\n",
    "* Leverage information from raw text\n",
    "* Unsupervised learning --> self-supervised learning\n",
    "    * extract and use the metadata and relevant context as supervisory signal.\n",
    "    - Learning setting up self-referential prediction tasks.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language models (1) - Word2Vec\n",
    "Mikolov et al. 2013: [\"Efficient Estimation of Word Representations in Vector Space\"](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "**Self-supervisory tasks** \n",
    "- Skip-Gram:\n",
    "    -Given a word: predict the previous and the next *k* words. \n",
    "    - Embed words in a contionous vector space and maximize cross-product between \"neighbor\" words. \n",
    "        - Essentially an efficient way of processing the NxN co-occurrence matrix.\n",
    "- CBOW: Contionous Bag of Words.\n",
    "    - Given a set of context words (before and after): predict the middle word.\n",
    "    - Maximize the cross-product between the middle word and the sum of its neighbors.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://skymind.ai/images/wiki/word2vec_diagrams.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1000/0*q4-VHtEoMwNGkanl.png)\n",
    "Image taken from https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec has been shown to:\n",
    "- **learns analogies** \n",
    "\n",
    "- **semantic and syntatic information** \n",
    "\n",
    "- **topical information** --> very efficient for lexicon extensions\n",
    "\n",
    "![](https://samyzaf.com/ML/nlp/word2vec2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Does not handle polysemi\n",
    "* each word has a single representation.\n",
    "    * \"$ bank\" and \"river bank\" share one representation. \n",
    "    \n",
    "\n",
    "Levy, Goldberg and Dagan 2016: [\"Improving Distributional Similaritywith Lessons Learned from Word Embeddings\"](https://www.aclweb.org/anthology/Q15-1016)\n",
    "- More classic methods of dimensionality reduction (e.g. SVD) are comparable, once Word2Vec \"tricks\" are applied.\n",
    "    - dynamic context window instead of co-occurrence based on document.\n",
    "    - smoothing methods (counts are raised to the power of 0.75)\n",
    "    - sampling methods (intelligent removal of very frequent words)\n",
    "    - add context vector (represent each word as its own vector plus average context vector)\n",
    "- But new \"Online\" Methods are far more efficient for estimation in Big Data settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num vectors</th>\n",
       "      <th>file size</th>\n",
       "      <th>base dataset</th>\n",
       "      <th>read_more</th>\n",
       "      <th>description</th>\n",
       "      <th>parameters</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conceptnet-numberbatch-17-06-300</td>\n",
       "      <td>1917247</td>\n",
       "      <td>1168 MB</td>\n",
       "      <td>ConceptNet, word2vec, GloVe, and OpenSubtitles...</td>\n",
       "      <td>http://aaai.org/ocs/index.php/AAAI/AAAI17/pape...</td>\n",
       "      <td>ConceptNet Numberbatch consists of state-of-th...</td>\n",
       "      <td>dimension - 300</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/commonsense/conceptnet-numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fasttext-wiki-news-subwords-300</td>\n",
       "      <td>999999</td>\n",
       "      <td>958 MB</td>\n",
       "      <td>Wikipedia 2017, UMBC webbase corpus and statmt...</td>\n",
       "      <td>https://fasttext.cc/docs/en/english-vectors.ht...</td>\n",
       "      <td>1 million word vectors trained on Wikipedia 20...</td>\n",
       "      <td>dimension - 300</td>\n",
       "      <td>-</td>\n",
       "      <td>https://creativecommons.org/licenses/by-sa/3.0/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glove-twitter-100</td>\n",
       "      <td>1193514</td>\n",
       "      <td>387 MB</td>\n",
       "      <td>Twitter (2B tweets, 27B tokens, 1.2M vocab, un...</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on 2B tweets, 27B to...</td>\n",
       "      <td>dimension - 100</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glove-twitter-200</td>\n",
       "      <td>1193514</td>\n",
       "      <td>758 MB</td>\n",
       "      <td>Twitter (2B tweets, 27B tokens, 1.2M vocab, un...</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on 2B tweets, 27B to...</td>\n",
       "      <td>dimension - 200</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glove-twitter-25</td>\n",
       "      <td>1193514</td>\n",
       "      <td>104 MB</td>\n",
       "      <td>Twitter (2B tweets, 27B tokens, 1.2M vocab, un...</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on 2B tweets, 27B to...</td>\n",
       "      <td>dimension - 25</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glove-twitter-50</td>\n",
       "      <td>1193514</td>\n",
       "      <td>199 MB</td>\n",
       "      <td>Twitter (2B tweets, 27B tokens, 1.2M vocab, un...</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on 2B tweets, 27B to...</td>\n",
       "      <td>dimension - 50</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>glove-wiki-gigaword-100</td>\n",
       "      <td>400000</td>\n",
       "      <td>128 MB</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on Wikipedia 2014 + ...</td>\n",
       "      <td>dimension - 100</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>glove-wiki-gigaword-200</td>\n",
       "      <td>400000</td>\n",
       "      <td>252 MB</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on Wikipedia 2014 + ...</td>\n",
       "      <td>dimension - 200</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>glove-wiki-gigaword-300</td>\n",
       "      <td>400000</td>\n",
       "      <td>376 MB</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on Wikipedia 2014 + ...</td>\n",
       "      <td>dimension - 300</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>glove-wiki-gigaword-50</td>\n",
       "      <td>400000</td>\n",
       "      <td>65 MB</td>\n",
       "      <td>Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)</td>\n",
       "      <td>https://nlp.stanford.edu/projects/glove/ https...</td>\n",
       "      <td>Pre-trained vectors based on Wikipedia 2014 + ...</td>\n",
       "      <td>dimension - 50</td>\n",
       "      <td>Converted to w2v format with python -m gensim....</td>\n",
       "      <td>http://opendatacommons.org/licenses/pddl/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>word2vec-google-news-300</td>\n",
       "      <td>3000000</td>\n",
       "      <td>1662 MB</td>\n",
       "      <td>Google News (about 100 billion words)</td>\n",
       "      <td>https://code.google.com/archive/p/word2vec/ ht...</td>\n",
       "      <td>Pre-trained vectors trained on a part of the G...</td>\n",
       "      <td>dimension - 300</td>\n",
       "      <td>-</td>\n",
       "      <td>not found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>word2vec-ruscorpora-300</td>\n",
       "      <td>184973</td>\n",
       "      <td>198 MB</td>\n",
       "      <td>Russian National Corpus (about 250M words)</td>\n",
       "      <td>https://www.academia.edu/24306935/WebVectors_a...</td>\n",
       "      <td>Word2vec Continuous Skipgram vectors trained o...</td>\n",
       "      <td>dimension - 300 window_size - 10</td>\n",
       "      <td>The corpus was lemmatized and tagged with Univ...</td>\n",
       "      <td>https://creativecommons.org/licenses/by/4.0/de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                name  num vectors file size  \\\n",
       "0   conceptnet-numberbatch-17-06-300      1917247   1168 MB   \n",
       "1    fasttext-wiki-news-subwords-300       999999    958 MB   \n",
       "2                  glove-twitter-100      1193514    387 MB   \n",
       "3                  glove-twitter-200      1193514    758 MB   \n",
       "4                   glove-twitter-25      1193514    104 MB   \n",
       "5                   glove-twitter-50      1193514    199 MB   \n",
       "6            glove-wiki-gigaword-100       400000    128 MB   \n",
       "7            glove-wiki-gigaword-200       400000    252 MB   \n",
       "8            glove-wiki-gigaword-300       400000    376 MB   \n",
       "9             glove-wiki-gigaword-50       400000     65 MB   \n",
       "10          word2vec-google-news-300      3000000   1662 MB   \n",
       "11           word2vec-ruscorpora-300       184973    198 MB   \n",
       "\n",
       "                                         base dataset  \\\n",
       "0   ConceptNet, word2vec, GloVe, and OpenSubtitles...   \n",
       "1   Wikipedia 2017, UMBC webbase corpus and statmt...   \n",
       "2   Twitter (2B tweets, 27B tokens, 1.2M vocab, un...   \n",
       "3   Twitter (2B tweets, 27B tokens, 1.2M vocab, un...   \n",
       "4   Twitter (2B tweets, 27B tokens, 1.2M vocab, un...   \n",
       "5   Twitter (2B tweets, 27B tokens, 1.2M vocab, un...   \n",
       "6    Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)   \n",
       "7    Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)   \n",
       "8    Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)   \n",
       "9    Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)   \n",
       "10              Google News (about 100 billion words)   \n",
       "11         Russian National Corpus (about 250M words)   \n",
       "\n",
       "                                            read_more  \\\n",
       "0   http://aaai.org/ocs/index.php/AAAI/AAAI17/pape...   \n",
       "1   https://fasttext.cc/docs/en/english-vectors.ht...   \n",
       "2   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "3   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "4   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "5   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "6   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "7   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "8   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "9   https://nlp.stanford.edu/projects/glove/ https...   \n",
       "10  https://code.google.com/archive/p/word2vec/ ht...   \n",
       "11  https://www.academia.edu/24306935/WebVectors_a...   \n",
       "\n",
       "                                          description  \\\n",
       "0   ConceptNet Numberbatch consists of state-of-th...   \n",
       "1   1 million word vectors trained on Wikipedia 20...   \n",
       "2   Pre-trained vectors based on 2B tweets, 27B to...   \n",
       "3   Pre-trained vectors based on 2B tweets, 27B to...   \n",
       "4   Pre-trained vectors based on 2B tweets, 27B to...   \n",
       "5   Pre-trained vectors based on 2B tweets, 27B to...   \n",
       "6   Pre-trained vectors based on Wikipedia 2014 + ...   \n",
       "7   Pre-trained vectors based on Wikipedia 2014 + ...   \n",
       "8   Pre-trained vectors based on Wikipedia 2014 + ...   \n",
       "9   Pre-trained vectors based on Wikipedia 2014 + ...   \n",
       "10  Pre-trained vectors trained on a part of the G...   \n",
       "11  Word2vec Continuous Skipgram vectors trained o...   \n",
       "\n",
       "                          parameters  \\\n",
       "0                    dimension - 300   \n",
       "1                    dimension - 300   \n",
       "2                    dimension - 100   \n",
       "3                    dimension - 200   \n",
       "4                     dimension - 25   \n",
       "5                     dimension - 50   \n",
       "6                    dimension - 100   \n",
       "7                    dimension - 200   \n",
       "8                    dimension - 300   \n",
       "9                     dimension - 50   \n",
       "10                   dimension - 300   \n",
       "11  dimension - 300 window_size - 10   \n",
       "\n",
       "                                        preprocessing  \\\n",
       "0                                                   -   \n",
       "1                                                   -   \n",
       "2   Converted to w2v format with python -m gensim....   \n",
       "3   Converted to w2v format with python -m gensim....   \n",
       "4   Converted to w2v format with python -m gensim....   \n",
       "5   Converted to w2v format with python -m gensim....   \n",
       "6   Converted to w2v format with python -m gensim....   \n",
       "7   Converted to w2v format with python -m gensim....   \n",
       "8   Converted to w2v format with python -m gensim....   \n",
       "9   Converted to w2v format with python -m gensim....   \n",
       "10                                                  -   \n",
       "11  The corpus was lemmatized and tagged with Univ...   \n",
       "\n",
       "                                              license  \n",
       "0   https://github.com/commonsense/conceptnet-numb...  \n",
       "1     https://creativecommons.org/licenses/by-sa/3.0/  \n",
       "2           http://opendatacommons.org/licenses/pddl/  \n",
       "3           http://opendatacommons.org/licenses/pddl/  \n",
       "4           http://opendatacommons.org/licenses/pddl/  \n",
       "5           http://opendatacommons.org/licenses/pddl/  \n",
       "6           http://opendatacommons.org/licenses/pddl/  \n",
       "7           http://opendatacommons.org/licenses/pddl/  \n",
       "8           http://opendatacommons.org/licenses/pddl/  \n",
       "9           http://opendatacommons.org/licenses/pddl/  \n",
       "10                                          not found  \n",
       "11  https://creativecommons.org/licenses/by/4.0/de...  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Download pretrained models\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import pandas as pd\n",
    "pretrained = pd.read_html('https://github.com/RaRe-Technologies/gensim-data')[-1]\n",
    "## Pretrained vectors \n",
    "'glove-twitter-200','glove-wiki-gigaword-300'\n",
    "pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "glove_200 = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('understood', 0.5150164365768433),\n",
       " ('أجِدُك', 0.38950029015541077),\n",
       " ('comprehend', 0.3834340274333954),\n",
       " ('understandable', 0.37860020995140076),\n",
       " ('understands', 0.37577465176582336),\n",
       " ('oblivious', 0.3744359016418457),\n",
       " ('explained', 0.3733295500278473),\n",
       " ('despise', 0.3595913052558899),\n",
       " ('understan', 0.35781174898147583),\n",
       " ('dgaf', 0.35316723585128784)]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_200.most_similar(positive = ['understand'],negative=['help'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('need', 0.8338778018951416),\n",
       " ('know', 0.8247804045677185),\n",
       " (\"n't\", 0.8162223696708679),\n",
       " ('how', 0.8040156960487366),\n",
       " ('tell', 0.801293134689331),\n",
       " ('can', 0.7996014356613159),\n",
       " ('trying', 0.7990914583206177),\n",
       " ('think', 0.7945455312728882),\n",
       " ('explain', 0.7894338369369507),\n",
       " ('could', 0.787102997303009)]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_200.most_similar(positive = ['understand','help'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4180768254479107"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_200.similarity('sad','warm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5067172400076395"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_200.similarity('sad','cold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language models (2) - From words to sequences\n",
    "- We need representation of meaning arising from sequential patterns.\n",
    "    - Context specific representations of each word (Polysemi)\n",
    "    - Sentence representations\n",
    "    \n",
    "**Self-Supervisory Tasks** \n",
    "- Given a sequence of characters: predict **next character** .\n",
    "    - Character-based Recurrent Neural Network (RNN). \n",
    "    - Takes a really long time to train, and model needs ability to model very long dependencies.\n",
    "    - Open question: Is characters a good fundamental unit for meaning? \n",
    "\n",
    "ELMO\n",
    "- Given a sequence of words: Predict **next (sub-)word**.\n",
    "    - Bi-directional: Reverse sequence prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelling sequential information\n",
    "\n",
    "**Forward LM architecture**\n",
    "![](http://www.davidsbatista.net/assets/images/2018-12-06-general_word_language_model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language models (3) - From words to sequences\n",
    "Innovations involve:\n",
    "- **Architecture design** \n",
    "    - Bi-directional: Both forward and backward dependencies.\n",
    "    - Attention: Add a layer to to \"analyze the sequence of hidden states\".\n",
    "        - Which hidden states are important to each other.\n",
    "    - Transformers: Dependencies between words not just sequential and memory-based (i.e. hidden state) but directly connected. \n",
    "    - To incorporate longer term sequential information. \n",
    "\n",
    "\n",
    "- Size of Model: From ELMo (93 million) to GPT2 (1.5billion)\n",
    "- Quality and size of dataset.\n",
    "    - Domain specific: Wikipedia, news corpuses, review data.\n",
    "    - Common Crawl. Large Dataset of Webpages.\n",
    "    - The [WEBTEXT](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) corpus GPT2 is trained on. 40 gb of outgoing and upvoted links from reddit. \n",
    "\n",
    "\n",
    "- ** Self-Supervisory Tasks ** \n",
    "    - BERT\n",
    "        - Given sequence of words: Predict **\"masked\"/missing word**.\n",
    "\n",
    "        - Predict **next sentence** and reverse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## State-of-the-art\n",
    "- Highly competitive field\n",
    "- Rapid discovery and advancement\n",
    "\n",
    "- High Compute and Large Models.\n",
    "    - ELMo: 93 million \n",
    "    - BERT: 240 million\n",
    "    - GPT2: 1.5 billion\n",
    "\n",
    "\n",
    "- Many complex model choices\n",
    "\n",
    "For kleeping up: Twitter and [Leaderboards](https://paperswithcode.com/task/sentiment-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## State-of-the-art (2)\n",
    "- 2017\n",
    "    - ELMo - Embeddings from Language Models. [\"Deep Contextualized word representations\"](https://arxiv.org/abs/1802.05365)\n",
    "- 2018\n",
    "    - [BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "    - [ULMFiT: Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)\n",
    "        - How to do Transfer Learning in NLP\n",
    "- 2019\n",
    "BERT + Data Augmentation --> Few shot learning.\n",
    "[\"Unsupervised Data Augmentation\"](https://arxiv.org/abs/1904.12848)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/abjer/tsds/master/material/13_text3/uda_state.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained models are available online.\n",
    "- [DeepMoji](https://github.com/bfelbo/DeepMoji)\n",
    "- [ELMo](https://github.com/HIT-SCIR/ELMoForManyLangs) for many languages\n",
    "- [BERT](https://github.com/google-research/bert) - Includes Multilingual Model\n",
    "    - [COLAB Example](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "\n",
    "[PyTorch implementation of the major LM](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias in NLP - from an applied social science perspective\n",
    "Social scientists adopt transfer learning\n",
    "\n",
    "### Sucess in few shot learning --> lower cost for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias in NLP - from an applied social science perspective (2)\n",
    "\n",
    "### Sucess in few shot learning --> means lower cost for training --> finally ressources for bias detection and correction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias in NLP\n",
    "\"The assumptions of random sampling is violated\" \n",
    "- Control over the sampling is lost using pretrained language models.\n",
    "\t\n",
    "- Model is greedy, picks up any association / pattern.\n",
    "\n",
    "**Examples**\n",
    "- **People express biases**\n",
    "    - Model learns correlations and association which are non-causal.\n",
    "    - Bolukbasi et al. 2016: [\"Man is to woman as computer programmer is to homemaker\"](https://arxiv.org/abs/1607.06520)\n",
    "    - Manzini et al. 2019: [\"Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings\"](https://arxiv.org/abs/1904.04047)\n",
    "    \n",
    "\n",
    "- **Different people express themselves differently.**\n",
    "    - Model learns styles \n",
    "    - Can be used for author attribution\n",
    "        - geo located (privacy)\n",
    "        - gender\n",
    "        - ethnicity\n",
    "    - Johansen, Hovy and Søgaard 2015: [\"Cross-lingual syntactic variation over age and gender\"](https://www.aclweb.org/anthology/K15-1011)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cops', 0.7516576051712036),\n",
       " ('officers', 0.6661906838417053),\n",
       " ('arrested', 0.6204742193222046),\n",
       " ('suspect', 0.6187559366226196),\n",
       " ('cop', 0.6156525015830994)]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_200.most_similar(positive=['police','black'],negative=['white'])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cops', 0.7516659498214722),\n",
       " ('officers', 0.7105646133422852),\n",
       " ('authorities', 0.6782428026199341),\n",
       " ('arrest', 0.6773560047149658),\n",
       " ('officials', 0.662535548210144)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_200.most_similar(positive=['police','white'],negative=['black'])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias NLP - Biased Measurement\n",
    "#### Using NLP systems for measurement introduce bias.\n",
    "- Hovy and Søgaard 2015: [\"Tagging performance correlates with author age\"](https://www.aclweb.org/anthology/P15-2079)\n",
    "    - Parsers were trained on old newspaper data.\n",
    "\n",
    "\n",
    "- Jørgensen, Hovy and Søgaard 2015: [\"Challenges of studying and processing dialects in social media\"](https://www.aclweb.org/anthology/W15-4302)\n",
    "    - Parsers were significantly worse in relation to african american dialect.\n",
    "    - --> NLP technology systematically  disadvantages  groups  of  non-standard language users.\n",
    "     \n",
    "     \n",
    "- Kiritchenko & Mohammad 2018: [\"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\"](https://arxiv.org/abs/1805.04508)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/abjer/tsds/master/material/13_text3/aave_dialect.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(lang='en')\n",
    "doc = nlp('ma brotha and ma sistas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Biased NLP \n",
    "**Few shot learning --> Potentially Prejudiced Classification** \n",
    "\n",
    "**Example**  \n",
    "\n",
    "**Job application, parole application**\n",
    "- African american style or gender makers, is easier to spot than making a Rational Decision\n",
    "    - spotting correlates to denied applications will give the model some performance, but in a biased manner.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Considerations\n",
    "- Bender and Friedman: [\"Data Statements for Natural Language Processing:Toward Mitigating System Bias and Enabling Better Science\"](https://openreview.net/pdf?id=By4oPeX9f)\n",
    "    - How was the dataset constructed?\n",
    "    - Are all categories of interest equally represented.\n",
    "        - performance might correlate with number of examples --> rare case and rare category problem. \n",
    " \n",
    "**Remedies** \n",
    "- Enough training data to ensure substantive instead of prejudice jugdement.\n",
    "- Stratified random sample on parameters of interest for training.\n",
    "- Stratified sampling on parameters of interest for Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias datasets\n",
    "Here we download the datasets we will use to investigate bias for tommorows exercise.\n",
    "\n",
    "From the paper:  \"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\" by Kiritchenko & Mohammad 2018:. [data](https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip)\n",
    "\n",
    "Kaggle Toxicity Classification: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n",
    "Follow the url. Sign in and download the zip file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip')\n",
    "with open('Equity-Evaluation-Corpus.zip','wb') as f:\n",
    "    f.write(response.content)\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('Equity-Evaluation-Corpus.zip', 'r')\n",
    "directory_to_extract_to = 'bias_dataset'\n",
    "import os\n",
    "if not os.path.isdir(directory_to_extract_to):\n",
    "    os.mkdir(directory_to_extract_to)\n",
    "zip_ref.extractall(directory_to_extract_to)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bias_df = pd.read_csv(directory_to_extract_to+'/Equity-Evaluation-Corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(European            2880\n",
       " African-American    2880\n",
       " Name: Race, dtype: int64, female    4320\n",
       " male      4320\n",
       " Name: Gender, dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_df.Race.value_counts(),bias_df.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Sentence', 'Template', 'Person', 'Gender', 'Race', 'Emotion',\n",
       "       'Emotion word'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1804874, 45), (97320, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "toxicity_df = pd.read_csv('/home/snorre/Dropbox/Forskning/PhD/undervisning/train.csv')\n",
    "toxicity_df_test = pd.read_csv('/home/snorre/Dropbox/Forskning/PhD/undervisning/test.csv')\n",
    "toxicity_df.shape,toxicity_df_test.shape\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (full)",
   "language": "python",
   "name": "env_full"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
