{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Analyzing Text\n",
    "## Characterizing the R and Python Communities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs\n",
    "import requests\n",
    "import wordcloud\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This pulls Snorre's expore_regex script from github \n",
    "# useful to set up an explorer that can be used to iteratively\n",
    "# find good regular expressions\n",
    "#\n",
    "# use the module to develop the pattern and finally use the .report() method to document your process. \n",
    "\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(requests.get('https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py').text)\n",
    "    \n",
    "import explore_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "# Load overflow data\n",
    "overflow_df = pd.read_csv('Posts_overflow.csv').dropna(subset = ['Body'])\n",
    "# Load the datascience data. \n",
    "datascience_df = pd.read_csv('Posts_ds.csv').dropna(subset = ['Body'])\n",
    "# load stats data\n",
    "stats_df = pd.read_csv('Posts_stats.csv').dropna(subset = ['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex.11.1.1** _Cleaning_\n",
    "First task is as with any other data experience (especially text data): It needs Cleaning. Here we need to brush up on our string and scraping fundamentals: Regular expressions and BeautifulSoup.\n",
    "\n",
    "Before we apply BeatifulSoup to get the clean text from HTML. We need to do some \"pre-preprocessing\".\n",
    "We shall take advantage of the HTML tags to extract code from text.\n",
    "\n",
    "Extract the code segments using the HTML tag: \"<\\CODE>\", and put it into a separate columns for later analysis.\n",
    "\n",
    "- Design a regular expression that match anything inside a `<code></code>` html-tag. \n",
    "> _Hint:_ inspect some of the data to see exactly how the code is wrapped in the html.\n",
    "- Use it to extract it to another column.\n",
    "- Use it to remove the code from the Body column.\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 11.1.2** Extract comments within the code using regular expressions matching everything between a **#** and a **\\n** \n",
    "\n",
    "Later we shall use this to analyze properties of the comment in relation to the code (e.g. number of comments,  length). Specifically we need to define a regular expressions that extracts comment text from within the code segments, by matching anything after # until a newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 11.1.3**\n",
    "Now that the code is removed we can now use BeautifulSoup to Extract the text from the HTML.\n",
    "> *Hint:* To use BeautifulSoup to extract the text from the Body columns use  `.get_text()`.\n",
    "\n",
    "Inspect if the results are good by sampling a few examples and skimming them. Wrap it all in a function and run it on all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.1.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex.11.1.4** Finally Extract the individual tags from the Tags columns using regular expressions. \n",
    "- Design a regular expression that matches the characters and symbols within the <> brackets. <[symbolsgoeshere]>, and assign it to a column named `tags_l` that holds a list of tags. \n",
    "- Count the Tags and Visualize them using the WordCloud package (https://github.com/amueller/word_cloud ). See this simple example: https://github.com/amueller/word_cloud/blob/master/examples/simple.py \n",
    "> *Hint:* use the method: .generate_from_frequencies(). That takes a dictionary of strings and their counts as input.\n",
    "\n",
    "**Extra:** \n",
    "Visualize two tag sets:\n",
    "    - one that co-occur with the <r> tag\n",
    "    - and another for the <python>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.1.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Filtering the data\n",
    "All analyses fundamentally depend on the way that the data was sampled. In this case I constructed a \"rough\" dataset, using a greedy matching of all R and Python patterns in the post and tags of the stackoverflow forums. Especially the matching of R was not completely unproblematic, and needs a re-iteration. \n",
    "\n",
    "There are two strategies here:\n",
    "- Work on a better Regular expression to match all instances of the R programming language and exclude false, if possible. \n",
    "- Or use the Tags column as a strategy for delimiting your data, using the userdefined Tags as qualifiers. Here it is about locating and delimiting which tags to include.\n",
    "    \n",
    "We will work on the latter strategy, but choices here could essentially have a profound effect on our results and could be part of a sensitivity analysis.\n",
    "\n",
    "A simple solution would be to Filter only Posts and \"children\" with tagged with \n",
    "- `<r>`\n",
    "- `<python>`\n",
    "\n",
    "However an initial analysis of the tagging behavior suggests that other programs and individual R packages are also used.\n",
    "\n",
    "To expand the tags for our sampling strategy we can use the same methodology as we shall use later for finding phrases and colocations. \n",
    "Here we shall use the PMI measure of association to expand our seeds.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Ex.11.2.1** Locate Tags to be used for delimiting our population using the PMI measure of association between all tags co-occurring with the <r> tag. \n",
    "\n",
    "- Use the tag_l column and count all co-occuring tags.\n",
    "- *Hint:* You can use itertools.combinations(tag_list,2) to iterate through co-occurring pairs and count them. \n",
    "\n",
    "Although we could use a networkx undirected graph as datastructure, this is a little overkill when not doing more advanced network analysis. Instead use the builtin Counter module (collections.Counter) and count the tuple pair (tag,tag2). As we are not interested in the direction(although sequence might tell you something), remember to sort the tags before counting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 11.2.2** Define a function with 4 arguments (token_count1,token_count2,cooc, N): Where N is the number of tag occurences in the corpus, and cooc is the co-occurence count. Use this input to compute the following formula:\n",
    "$$\\operatorname{pmi}(x ; y) \\equiv \\log \\frac{p(x, y)}{p(x) p(y)}$$\n",
    "- Compute the PMI of all tag pairs with the the `<r>` tag present.\n",
    "- Inspect the top and the bottom to see how well the measure does.\n",
    "- Define a variable r_tags as a set() of tags learned by inspecting the top 50 pmi scored tags.   \n",
    "- Finally make it re-usable by wrapping the whole thing in a function that takes a list of tokens as input, and returns counters of individudual tokens, and co-occurring tokens along. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to ex. 11.2.2 here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
